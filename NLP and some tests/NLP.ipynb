{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bjWsrMZ3JPr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_news = pd.read_csv('/content/news.csv').astype(str)\n",
        "df_news.text[0]"
      ],
      "metadata": {
        "id": "SP8ZUQJR3T-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_corpus(corpus):\n",
        "    str_corpus = ''\n",
        "    for i in corpus:\n",
        "        str_corpus += ' ' + i\n",
        "    str_corpus = str_corpus.strip()\n",
        "    return str_corpus\n",
        "\n",
        "def get_corpus(data):\n",
        "    corpus = []\n",
        "    for phrase in data:\n",
        "        for word in phrase.split():\n",
        "            corpus.append(word)\n",
        "    return corpus\n",
        "\n",
        "def get_wordCloud(corpus):\n",
        "    wordCloud = WordCloud(background_color='white',\n",
        "                              stopwords=STOPWORDS,\n",
        "                              width=3000,\n",
        "                              height=2500,\n",
        "                              max_words=200,\n",
        "                              random_state=42\n",
        "                         ).generate(str_corpus(corpus))\n",
        "    return wordCloud\n",
        "\n",
        "corpus = get_corpus(df_news['text'].values)\n",
        "procWordCloud = get_wordCloud(corpus)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(procWordCloud)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 1)"
      ],
      "metadata": {
        "id": "k_OhwNZb4Nbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(set(corpus))\n",
        "num_words"
      ],
      "metadata": {
        "id": "Ujmtq-4B6Fml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news.to_csv('news_try.csv', index=False)\n",
        "df_news_try = pd.read_csv('/content/news_try.csv')\n",
        "df_news_try.text[0]\n"
      ],
      "metadata": {
        "id": "lAwxDW9juBQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = {33: ' ', 34: ' ', 35: ' ', 36: ' ', 37: ' ', 38: ' ', 39: ' ', 40: ' ', 41: ' ', 42: ' ', 43: ' ', 44: ' ',\n",
        "             45: ' ', 46: ' ', 47: ' ', 58: ' ', 59: ' ', 60: ' ', 61: ' ', 62: ' ', 63: ' ', 64: ' ', 91: ' ',\n",
        "             92: ' ', 93: ' ', 94: ' ', 95: ' ', 96: ' ', 123: ' ', 124: ' ', 125: ' ', 126: ' '}\n",
        "    return text.translate(table)\n",
        "\n",
        "\n",
        "df_news_try['text_clean'] = df_news_try['text'].map(lambda x: x.lower())\n",
        "df_news_try['text_clean'] = df_news_try['text_clean'].map(lambda x: remove_punct(x))\n",
        "df_news_try['text_clean'] = df_news_try['text_clean'].map(lambda x: x.split(' '))\n",
        "df_news_try['text_clean'] = df_news_try['text_clean'].map(lambda x: [token for token in x if token not in russian_stopwords\\\n",
        "                                                                  and token != \" \" \\\n",
        "                                                                  and token.strip() not in punctuation])\n",
        "df_news_try['text_clean'] = df_news_try['text_clean'].map(lambda x: ' '.join(x))\n",
        "\n",
        "df_news_try.text_clean[1]"
      ],
      "metadata": {
        "id": "ZcqBQXYisfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_try.text_clean[1]"
      ],
      "metadata": {
        "id": "RJ-EcoQxxhrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def str_corpus(corpus):\n",
        "    str_corpus = ''\n",
        "    for i in corpus:\n",
        "        str_corpus += ' ' + i\n",
        "    str_corpus = str_corpus.strip()\n",
        "    return str_corpus\n",
        "\n",
        "def get_corpus(data):\n",
        "    corpus = []\n",
        "    for phrase in data:\n",
        "        for word in phrase.split():\n",
        "            corpus.append(word)\n",
        "    return corpus\n",
        "\n",
        "def get_wordCloud(corpus):\n",
        "    wordCloud = WordCloud(background_color='white',\n",
        "                              stopwords=STOPWORDS,\n",
        "                              width=3000,\n",
        "                              height=2500,\n",
        "                              max_words=200,\n",
        "                              random_state=42\n",
        "                         ).generate(str_corpus(corpus))\n",
        "    return wordCloud\n",
        "\n",
        "corpus = get_corpus(df_news_try['text_clean'].values)\n",
        "procWordCloud = get_wordCloud(corpus)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(procWordCloud)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 1)"
      ],
      "metadata": {
        "id": "x6N29t9-x_XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_news_try.head(3)"
      ],
      "metadata": {
        "id": "YuJVOconyUq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(df_news_try['text_clean'], df_news_try['rubric'], test_size=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "uZXu3qTo6pi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "D4zSKZiPhKib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "sgd_ppl_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('sgd_clf', SGDClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "'''knb_ppl_clf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('knb_clf', KNeighborsClassifier(n_neighbors=4))\n",
        "])'''\n",
        "\n",
        "\n",
        "sgd_ppl_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "G1fPVP_f9I1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_sgd = sgd_ppl_clf.predict(X_test)\n",
        "print(metrics.classification_report(predicted_sgd, y_test))\n"
      ],
      "metadata": {
        "id": "K9xsMTKL9aP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_sgd_val = sgd_ppl_clf.predict(X_valid)\n",
        "print(metrics.classification_report(predicted_sgd_val, y_valid))"
      ],
      "metadata": {
        "id": "ASPTG6Y9i1BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "PzK1OKtihh9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(sgd_ppl_clf, 'learned_model.pkl')"
      ],
      "metadata": {
        "id": "8szH7HzVnLsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}